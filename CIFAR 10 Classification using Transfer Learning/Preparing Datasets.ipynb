{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Transfer Learning on CIFAR-10 Dataset\n",
    "\n",
    "## Part 1: Preparing the Dataset\n",
    "The dataset contains a folder *train* and a CSV file *TrainLabel.csv* for labeling each images. We will combine them into one training dataset and store the image as *Features Set* and label as *Classes Set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Images and Adding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▌                                                                   | 5000/50000 [00:02<00:22, 2015.44it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = read_csv(\"trainLabels.csv\")\n",
    "classes = sorted(list(set(labels.label.values)))\n",
    "dataset = []\n",
    "y = []\n",
    "image_size = (224, 224)\n",
    "\n",
    "DATASET_DIR = \"D:\\Computer Vision\\CIFAR 10 Classification using Transfer Learning\\\\train\"\n",
    "batch_size = 5000\n",
    "count = 0\n",
    "\n",
    "\n",
    "for file in tqdm(os.listdir(os.path.join(DATASET_DIR)[:5000])):\n",
    "    if count < batch_size:\n",
    "        filename = int(file.split(\".\")[0])\n",
    "        class_index = classes.index(labels[labels['id']==filename].label.item())\n",
    "        y.append(class_index)\n",
    "        image = cv2.imread(os.path.join(DATASET_DIR, file))\n",
    "        resized = cv2.resize(image, image_size)\n",
    "        dataset.append(resized)\n",
    "        count+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Datasets and Classes into Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Features and Labels in File for Further Training of the Neural Network\n",
    "We will save the features and labels in separate files so that we don't need to prepare our datasets everytime we need to run training codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = open(\"features.pickle\", \"wb\")\n",
    "pickle.dump(dataset, features)\n",
    "features.close()\n",
    "\n",
    "labels = open(\"labels.pickle\", \"wb\")\n",
    "pickle.dump(y, labels)\n",
    "labels.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
